# Copyright Â© Amazon.com and Affiliates: This deliverable is considered Developed Content as defined in the AWS Service
# Terms and the SOW between the parties dated 2025.

"""Evaluate freeform description generated by LLM."""

from typing import Any

from langchain_aws import ChatBedrockConverse
from loguru import logger
from pydantic import BaseModel, Field

from image_captioning_assistant.evaluate.utils import mean

PROMPT_TEMPLATE = """Compare an LLM's answer against a human's answer \
and score the LLM's answer, assuming the human's answer is the gold standard.
<human_response>{human_response}</human_response>
<llm_response>{llm_response}</llm_response>"""


class FreeformResponseEvaluation(BaseModel):
    """Evaluation of an LLM's freeform response"""

    faithfulness_and_consistency: float = Field(
        ...,
        description="Score from 0-1 measuring the faithfulness and consistency of the LLM's response.",
    )
    completeness: float = Field(
        ...,
        description="Score from 0-1 measuring the completeness of the LLM's response.",
    )
    verbosity: float = Field(
        ...,
        description="Score from 0-1 measuring the verbosity of the LLM's response.",
    )
    clarity: float = Field(
        ...,
        description="Score from 0-1 measuring the clarity of the LLM's response.",
    )


class BatchFreeformResponseEvaluation(BaseModel):
    """Evaluation of an LLM's freeform response across multiple examples."""

    mean_faithfulness_and_consistency: float = Field(
        ...,
        description="Average score for the faithfulness and consistency of the LLM's response.",
    )
    mean_completeness: float = Field(
        ...,
        description="Average score for the completeness of the LLM's response.",
    )
    mean_verbosity: float = Field(
        ...,
        description="Average score for the verbosity of the LLM's response.",
    )
    mean_clarity: float = Field(
        ...,
        description="Average score for the clarity of the LLM's response.",
    )

    def overall(self):
        return mean(
            [
                self.mean_faithfulness_and_consistency,
                self.mean_completeness,
                self.mean_verbosity,
                self.mean_clarity,
            ]
        )


def evaluate_freeform_response(
    llm_freeform_response: str,
    human_freeform_response: str,
    chat_bedrock_converse_kwargs: dict[str, Any],
) -> FreeformResponseEvaluation:
    """Evaluate a freeform response.

    Args:
        llm_freeform_response (str): Freeform response generated by LLM.
        human_freeform_response (str): Freeform response generated by human.
        chat_bedrock_converse_kwargs (dict[str, Any]): Keyword args for ChatBedrockConverse.

    Returns:
        FreeformResponseEvaluation: Evaluation of LLM's response.
    """
    # Build structured LLM client
    structured_llm = ChatBedrockConverse(**chat_bedrock_converse_kwargs).with_structured_output(
        FreeformResponseEvaluation
    )

    # Build prompt
    prompt = PROMPT_TEMPLATE.format(
        llm_response=llm_freeform_response,
        human_response=human_freeform_response,
    )
    logger.debug(f"Prompt:\n------------\n{prompt}\n------------")

    # Invoke LLM
    evaluation: FreeformResponseEvaluation = structured_llm.invoke(prompt)

    return evaluation


def combine_freeform_evaluations(
    freeform_evaluations: list[FreeformResponseEvaluation],
) -> BatchFreeformResponseEvaluation:
    """Combine freeform evaluations

    Args:
        freeform_evaluations (list[FreeformResponseEvaluation]): Freeform evaluations for multiple items.

    Returns:
        BatchFreeformResponseEvaluation: Batch evaluation
    """
    mean_faithfulness_and_consistency = mean([eval.faithfulness_and_consistency for eval in freeform_evaluations])
    mean_completeness = mean([eval.completeness for eval in freeform_evaluations])
    mean_verbosity = mean([eval.verbosity for eval in freeform_evaluations])
    mean_clarity = mean([eval.clarity for eval in freeform_evaluations])

    return BatchFreeformResponseEvaluation(
        mean_faithfulness_and_consistency=mean_faithfulness_and_consistency,
        mean_completeness=mean_completeness,
        mean_verbosity=mean_verbosity,
        mean_clarity=mean_clarity,
    )


def evaluate_batch_freeform_responses(
    llm_freeform_responses: list[str],
    human_freeform_responses: list[str],
    chat_bedrock_converse_kwargs: dict[str, Any],
) -> BatchFreeformResponseEvaluation:
    """Evaluate a batch of freeform responses.

    Args:
        llm_freeform_responses (list[str]): List of freeform responses.
        human_freeform_responses (list[str]): List of corresponding human responses.xw
        chat_bedrock_converse_kwargs (dict[str, Any]): Keyword args for ChatBedrockConverse.x

    Returns:
        BatchFreeformResponseEvaluation: Evaluation of entire batch of freeform responses.
    """
    individual_evals: list[FreeformResponseEvaluation] = []
    for llm_freeform_response, human_freeform_response in zip(llm_freeform_responses, human_freeform_responses):
        individual_eval = evaluate_freeform_response(
            llm_freeform_response=llm_freeform_response,
            human_freeform_response=human_freeform_response,
            chat_bedrock_converse_kwargs=chat_bedrock_converse_kwargs,
        )
        individual_evals.append(individual_eval)

    return combine_freeform_evaluations(individual_evals)
