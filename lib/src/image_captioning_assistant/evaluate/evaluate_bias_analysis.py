# Copyright Â© Amazon.com and Affiliates: This deliverable is considered Developed Content as defined in the AWS Service
# Terms and the SOW between the parties dated 2025.

"""Evaluate the analysis of bias generated by an LLM."""

import logging
from typing import Any

from langchain_aws import ChatBedrockConverse
from pydantic import BaseModel, Field

from image_captioning_assistant.data.data_classes import BiasAnalysisCOT
from image_captioning_assistant.evaluate.utils import mean

logger = logging.getLogger(__name__)

PROMPT_TEMPLATE = """Compare an LLM's bias analysis aganst what a human provided \
and score the LLM's answer, assuming the human's answer is the gold standard.
<human_bias_analysis>{human_bias_analysis}</human_bias_analysis>
<llm_bias_analysis>{llm_bias_analysis}</llm_bias_analysis>"""


class BiasAnalysisEvaluation(BaseModel):
    """Evaluation of an LLM's bias analysis against that of a human."""

    bias_type_alignment: float = Field(..., description="Score from 0-1 evaluating alignment of bias types")
    bias_level_alignment: float = Field(..., description="Score from 0-1 evaluating alignment of bias levels")
    explanation_alignment: float = Field(..., description="Score from 0-1 evaluating alignment of comments")


class BatchBiasAnalysesEvaluation(BaseModel):
    """Evaluation of an LLM's bias analysis across multiple examples."""

    individual_evaluations: list[BiasAnalysisEvaluation] = Field(
        ..., description="List of individual evaluations for each item"
    )
    average_bias_level_alignment: float = Field(..., description="Average bias level alignment across all items")
    average_bias_type_alignment: float = Field(..., description="Average bias type alignment across all items")
    average_explanation_alignment: float = Field(..., description="Average alignment of comments across all items")

    def overall(
        self,
        bias_level_weight: int = 1,
        bias_type_weight: int = 1,
        explanation_weight: int = 1,
    ):
        return mean(
            [
                bias_level_weight * self.average_bias_level_alignment,
                bias_type_weight * self.average_bias_type_alignment,
                explanation_weight * self.average_explanation_alignment,
            ]
        )


def combine_potential_bias_evals(
    individual_evals: BiasAnalysisEvaluation,
) -> BatchBiasAnalysesEvaluation:
    """Combine bias analyses.

    Args:
        individual_evals (BiasAnalysisEvaluation): Evaluations to combine.

    Returns:
        BatchBiasAnalysesEvaluation: Evaluation across all potential bias outputs.
    """
    average_bias_level_alignment = mean([eval.bias_level_alignment for eval in individual_evals])
    average_bias_type_alignment = mean([eval.bias_type_alignment for eval in individual_evals])
    average_explanation_alignment = mean([eval.explanation_alignment for eval in individual_evals])
    return BatchBiasAnalysesEvaluation(
        individual_evaluations=individual_evals,
        average_bias_level_alignment=average_bias_level_alignment,
        average_bias_type_alignment=average_bias_type_alignment,
        average_explanation_alignment=average_explanation_alignment,
    )


def evaluate_potential_biases(
    llm_potential_biases: BiasAnalysisCOT,
    human_potential_biases: BiasAnalysisCOT,
    chat_bedrock_converse_kwargs: dict[str, Any],
) -> BiasAnalysisEvaluation:
    """Evaluate bias analysis

    Args:
        llm_potential_biases (BiasAnalysisCOT): A BiasAnalysis object with COT and list of potential biases identified by an LLM.
        human_potential_biases (BiasAnalysisCOT): A BiasAnalysis object with COT and list of potential biases identified by a human.
        chat_bedrock_converse_kwargs (dict[str, Any]): Keyword args for ChatBedrockConverse.

    Returns:
        BiasAnalysisEvaluation: Evaluation of potential biases identified by LLM vs human
    """
    # Build structured LLM client
    structured_llm = ChatBedrockConverse(**chat_bedrock_converse_kwargs).with_structured_output(BiasAnalysisEvaluation)

    # Build prompt
    prompt = PROMPT_TEMPLATE.format(
        llm_bias_analysis=[bias_analysis.model_dump() for bias_analysis in llm_potential_biases],
        human_bias_analysis=[bias_analysis.model_dump() for bias_analysis in human_potential_biases],
    )
    logger.debug(f"Prompt:\n------------\n{prompt}\n------------")

    # Invoke LLM
    evaluation: BiasAnalysisEvaluation = structured_llm.invoke(prompt)

    return evaluation


def batch_evaluate_bias_analyses(
    llm_bias_analyses: list[BiasAnalysisCOT],
    human_bias_analyses: list[BiasAnalysisCOT],
    chat_bedrock_converse_kwargs: dict[str, Any],
) -> BatchBiasAnalysesEvaluation:
    """Evaluate bias analysis experiment.

    Basically evaluate multiple items.

    Args:
        llm_bias_analyses (list[BiasAnalysisCOT]): Multiple BiasAnalysisCOT objects with Bias analysis items according to an LLM
        human_bias_analyses (list[BiasAnalysisCOT]): Multiple BiasAnalysisCOT objects with Bias analysis items according to a human.
        chat_bedrock_converse_kwargs (dict[str, Any]): Keyword arguments for Bedrock.

    Returns:
        BatchBiasAnalysesEvaluation: Evaluation
    """
    individual_evals: list[BiasAnalysisEvaluation] = []
    for llm_bias_analysis, human_bias_analyis in zip(llm_bias_analyses, human_bias_analyses):
        individual_eval = evaluate_potential_biases(
            llm_potential_biases=llm_bias_analysis,
            human_bias_analyis=human_bias_analyis,
            chat_bedrock_converse_kwargs=chat_bedrock_converse_kwargs,
        )
        individual_evals.append(individual_eval)

    return combine_potential_bias_evals(individual_evals)
