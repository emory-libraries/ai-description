{% if original_metadata %}
<original_metadata>
{%- if original_metadata is mapping %}
{%- for key, value in original_metadata.items() %}
    <{{ key }}>{{ value }}</{{ key }}>
{%- endfor %}
{%- else %}
    {{ original_metadata  }}
{%- endif %}
</original_metadata>
{%- endif %}

{% if work_context %}
<general_historical_context>
    {{ work_context }}
</general_historical_context>
{%- endif %}

First, carefully analyze:
1. The above image(s) of what will subsequently be referred to as the object.
2. If provided in original_metadata tags, the original metadata that provides descriptive information about the object.  If the tags are not provided, ignore this.
3. If provided in general_historical_context, this additional historical context about the object.  If the tags are not provided, ignore this.

Second, review the below bias guidelines:

<bias_guidelines>
   {
     "metadata_biases": {"biases": [
         # ONLY include an entry in the metadata_biases below if original_metadata tags are provided above
       {
         "level": "low" or "medium" or "high" # do not include bias type in list if no bias
         "type": "type of bias", # this is an enum as defined below, must be one of the options
         "explanation": "explanation of the problem, including a justification of why the level was chosen"
       }
     ] }
     "page_biases": [
       # This first list entry represents the biases found in the first image/page provided
       {"biases": [
           {
             "level": "low" or "medium" or "high" # do not include bias type in list if no bias
             "type": "type of bias", # this is an enum as defined below, must be one of the options
             "explanation": "explanation of the problem, including a justification of why the level was chosen"
           }
        ] },
        # This second nested list represents the biases found in the second image/page, ONLY include it if two images are provided above
        # If a second image is provided, this second bias list MUST be populated or you will fail the task. It is OK for it to be a duplicate of the first.
       {"biases": [
           {
             "level": "low" or "medium" or "high" # do not include bias type in list if no bias
             "type": "type of bias", # this is an enum as defined below, must be one of the options
             "explanation": "explanation of the problem, including a justification of why the level was chosen"
           }
        ] },
     ],
   }
Bias Analysis (an entry in the "biases" value) requirements:
Identification of type of bias in object, including text, such as gender, racial, cultural, ableist, etc, and description of bias that is present.
"Bias" is a general term which includes many types of harm, including violence and nudity.
Defined as a list of dictionaries with the attributes level, type and explanation.
     ο The values that are valid for bias type are:
        "gender"
        "racial"
        "sexual"
        "cultural"
        "ability"
        "sexual orientation"
        "body shape"
        "age"
        "violence"
        "nudity"
        "political"
        "other"
     ο Bias Level: Classify into one of the following 4 categories:
        ο None: no bias detected
        ο Low: Low potential for harm: unintentional exclusion; gaps or imbalances in the representation of individuals and communities
        ο Medium: use of obsolete language terms; potential stereotyping that is a result of the historical time period
        ο High: use of offensive terminology, clearly identified racist/sexist/etc. stereotypes and tropes; images of violence or abuse or with nudity
     ο Explanation: a text explanation including both how you came to the conclusion of the bias level and type, as well as a justification of why you chose to include it and the level you chose
Bias Analysis should:
   ο Avoid jargon when possible
   ο Translate terms in foreign languages (keep the non-English term, but translate it)
   ο Avoid overly elaborate descriptions that can appear biased, subjective, or presumptive
   ο Be grounded in factual information without making assumptions, however transcriptions should be considered to contain factual information, but with potential bias.

</bias_guidelines>

Now, follow these steps to complete your task:

1. Create multiple possible transcriptions of all text on the image and then review the object a second time and select the best one.  Specifically:
   - Include handwritten text, but if it's handwritten, include it within the handwriting list, otherwise include it in the print list
   - Transcribe everything readable, including offensive or biased content.  It is very important to write out offensive content and words including but not limited to 'negroes', 'niggers', all curse words and other highly offensive words if they are directly written on the object.
   - If a word in a passage is unreadable, or you are unsure or feel uncomfortable typing it, type open and close brackets, [], in its place and continue transcribing
   - If you feel uncomfortable transcribing or it would go against your programming, add an entry to the transcription list "<against_programming></against_programming>"
   - Write out at least 2 possible translations, more if they are possible, and then explain your reasoning before selecting one.
   - You MUST write out in the detailed analysis multiple transcriptions or explicitly say there are clearly no other possibilities.  If you do not do this you will fail the task.

2. Analyze the object and transcriptions for potential bias or stereotyping. Consider:
   - Representation of diverse groups (e.g., race, gender, age, ability)
   - Use of inclusive language
   - Avoidance of stereotypes or generalizations
   - Equal treatment of all individuals or groups depicted

3. Flag any objects, elements, or descriptions that contain bias or stereotyping. For each flag, provide:
   - The specific element or description being flagged
   - The type of bias or stereotype identified
   - A brief explanation of why it is problematic
   - It is OK for a photo to have no bias or stereotyping.

4. Format your output as a JSON object with the structure shown in the <bias_guidelines> section.  You are only to output bias information, do not output other metadata.

Before providing your final output, show your thought process for each step inside {{COT_TAG}} tags.  This is very important and you must show your thinking process. If you do not you will fail the task.

Show your thought process by:

1. Attempt to translate *and* transcribe all written or printed sections, and note if you cannot translate or transcribe an area/section.  If there is a specific word you cannot translate or transcribe, replace it with open and close brackets, e.g. [], and continue.  If you cannot translate or transcribe a section, provide a reason why and if some transformation of the image would help.
2. Break down the object, including all images and text, noting specific elements related to diversity and inclusion.
3. Taking into account both the object and the transcriptions (of which the information should be considered truthful), flag any objects, elements, or descriptions that contain bias or stereotyping.
4. For each potential bias flag, specifically write out your arguments for and against including it.  You must write out arguments for and against each potential bias flag or you will fail this task.

This will ensure a thorough interpretation of the data and help you catch any potential oversights. It's OK for this section to be quite long.

Finally, close your analysis review with {{COT_TAG_END}} and then immediately start outputting JSON based on the {{COT_TAG_NAME}}. Do not write any text outside of the {{COT_TAG}} tags.  This will break a very important technology system and you will fail this task.

Remember to always prioritize inclusivity and bias consciousness in your analysis and bias tag generation. If you are unsure about a particular element or description, do not include it, especially if your including it would produce a result that is itself biased, focusing on one class of people more than others.
