"""Evaluate structured metadata generated by LLM."""

import json
from typing import Any

from langchain_aws import ChatBedrockConverse
from loguru import logger
from pydantic import BaseModel, Field

from image_captioning_assistant.data.data_classes import StructuredMetadata
from image_captioning_assistant.evaluate.evaluate_bias_analysis import (
    BatchBiasAnalysesEvaluation,
    BiasAnalysisEvaluation,
    combine_bias_analysis_evals,
    evaluate_bias_analysis,
)
from image_captioning_assistant.evaluate.evaluate_freeform_description import (
    BatchFreeformResponseEvaluation,
    FreeformResponseEvaluation,
    combine_freeform_evaluations,
    evaluate_freeform_response,
)
from image_captioning_assistant.evaluate.utils import mean

PROMPT_TEMPLATE = """Compare the structured image metadata generated by an LLM
against the structured metadata written by a human. \
Evaluate the LLM's performance, assuming the human's answer is the gold standard.
<human_written_metadata>{human_written_metadata}</human_written_metadata>
<llm_generated_metadata>{llm_generated_metadata}</llm_generated_metadata>"""


class PartialStructuredMetadataEvaluation(BaseModel):
    """Evaluation of many of the structured metadata fields generated by an LLM."""

    transcription_evaluation: float = Field(
        ..., description="Score from 0-1 evaluating LLM's transcription of image text"
    )
    names_evaluation: float = Field(
        ...,
        description="Score from 0-1 evaluating named individuals LLM found in image",
    )
    date_evaluation: float = Field(
        ..., description="Score from 0-1 evaluating date of image estimated by LLM"
    )
    location_evaluation: float = Field(
        ..., description="Score from 0-1 evaluating location of image estimated by LLM"
    )
    publication_info_evaluation: float = Field(
        ...,
        description="Score from 0-1 evaluating publication info for image generated by LLM",
    )
    contextual_info_evaluation: float = Field(
        ...,
        description="Score from 0-1 evaluating contextual info for image generated by LLM",
    )


class StructuredMetadataEvaluation(PartialStructuredMetadataEvaluation):
    """Evaluation of all structured metadata fields generated by an LLM."""

    description_evaluation: FreeformResponseEvaluation = Field(
        ..., description="Evaluation of LLM's description of image"
    )
    bias_analysis_evaluation: BiasAnalysisEvaluation = Field(
        ..., description="Evaluation of LLM's analysis of bias in image"
    )


class BatchStructuredMetadataEvaluation(BaseModel):
    """Evaluation of all structured metadata fields generated by an LLM across multiple items."""

    mean_transcription_evaluation: float = Field(
        ..., description="Average transcription evaluation score"
    )
    mean_names_evaluation: float = Field(
        ..., description="Average names evaluation score"
    )
    mean_date_evaluation: float = Field(
        ..., description="Average date evaluation score"
    )
    mean_location_evaluation: float = Field(
        ..., description="Average location evaluation score"
    )
    mean_publication_info_evaluation: float = Field(
        ..., description="Average publication info evaluation score"
    )
    mean_contextual_info_evaluation: float = Field(
        ..., description="Average contextual info evaluation score"
    )
    overall_description_evaluation: BatchFreeformResponseEvaluation = Field(
        ..., description="Overall evaluation of image description"
    )
    overall_bias_analysis_evaluation: BatchBiasAnalysesEvaluation = Field(
        ..., description="Overall evaluation of bias analysis"
    )

    def overall(
        self,
        description_weight: int = 1,
        transcription_weight: int = 1,
        names_weight: int = 1,
        date_weight: int = 1,
        location_weight: int = 1,
        publication_info_weight: int = 1,
        contextual_info_weight: int = 1,
        bias_weight: int = 1,
    ):
        return mean(
            [
                description_weight * self.overall_description_evaluation.overall(),
                transcription_weight * self.mean_transcription_evaluation,
                names_weight * self.mean_names_evaluation,
                date_weight * self.mean_date_evaluation,
                location_weight * self.mean_location_evaluation,
                publication_info_weight * self.mean_publication_info_evaluation,
                contextual_info_weight * self.mean_contextual_info_evaluation,
                bias_weight * self.overall_bias_analysis_evaluation.overall(),
            ]
        )


def evaluate_structured_metadata(
    llm_structured_metadata: StructuredMetadata,
    human_structured_metadata: StructuredMetadata,
    chat_bedrock_converse_kwargs: dict[str, Any],
) -> StructuredMetadataEvaluation:
    """Evaluate structured metadata generated by an LLM.

    Args:
        llm_structured_metadata (StructuredMetadata): Structured metadata generated by an LLM.
        human_structured_metadata (StructuredMetadata): Structured metadata written by a human.
        chat_bedrock_converse_kwargs (dict[str, Any]): Keyword args for ChatBedrockConverse.

    Returns:
        StructuredMetadataEvaluation: Evaluation of the structured metadata the LLM generated.
    """
    # Evaluate freeform image description
    freeform_response_evaluation: FreeformResponseEvaluation = (
        evaluate_freeform_response(
            llm_freeform_response=llm_structured_metadata.description,
            human_freeform_response=human_structured_metadata.description,
            chat_bedrock_converse_kwargs=chat_bedrock_converse_kwargs,
        )
    )
    # Evaluate bias
    bias_analysis_evaluation: BiasAnalysisEvaluation = evaluate_bias_analysis(
        llm_bias_analysis=llm_structured_metadata.bias_analysis,
        human_bias_analysis=human_structured_metadata.bias_analysis,
        chat_bedrock_converse_kwargs=chat_bedrock_converse_kwargs,
    )
    # Build structured LLM client
    structured_llm = ChatBedrockConverse(
        **chat_bedrock_converse_kwargs
    ).with_structured_output(PartialStructuredMetadataEvaluation)

    # Build prompt
    human_written_metadata_str = json.dumps(
        human_structured_metadata.model_dump(), indent=2
    )
    llm_generated_metadata_str = json.dumps(
        llm_structured_metadata.model_dump(), indent=2
    )
    prompt = PROMPT_TEMPLATE.format(
        human_written_metadata=human_written_metadata_str,
        llm_generated_metadata=llm_generated_metadata_str,
    )
    logger.debug(f"Prompt:\n------------\n{prompt}\n------------")

    # Invoke LLM
    partial_structured_metadata_evaluation: PartialStructuredMetadataEvaluation = (
        structured_llm.invoke(prompt)
    )

    return StructuredMetadataEvaluation(
        description_evaluation=freeform_response_evaluation,
        transcription_evaluation=partial_structured_metadata_evaluation.transcription_evaluation,
        names_evaluation=partial_structured_metadata_evaluation.names_evaluation,
        date_evaluation=partial_structured_metadata_evaluation.date_evaluation,
        location_evaluation=partial_structured_metadata_evaluation.location_evaluation,
        contextual_info_evaluation=partial_structured_metadata_evaluation.contextual_info_evaluation,
        publication_info_evaluation=partial_structured_metadata_evaluation.publication_info_evaluation,
        bias_analysis_evaluation=bias_analysis_evaluation,
    )


def combine_structured_metadata_evaluations(
    metadata_evaluations: list[StructuredMetadataEvaluation],
) -> BatchStructuredMetadataEvaluation:
    """Combine structured metadata evaluations.

    Args:
        metadata_evaluations (list[StructuredMetadataEvaluation]): List of metadata evaluations.

    Returns:
        BatchStructuredMetadataEvaluation: Overall metadata evaluation.
    """
    mean_contextual_info_evaluation = mean(
        [eval.contextual_info_evaluation for eval in metadata_evaluations]
    )
    mean_date_evaluation = mean([eval.date_evaluation for eval in metadata_evaluations])
    mean_location_evaluation = mean(
        [eval.location_evaluation for eval in metadata_evaluations]
    )
    mean_names_evaluation = mean(
        [eval.names_evaluation for eval in metadata_evaluations]
    )
    mean_publication_info_evaluation = mean(
        [eval.publication_info_evaluation for eval in metadata_evaluations]
    )
    mean_transcription_evaluation = mean(
        [eval.transcription_evaluation for eval in metadata_evaluations]
    )
    overall_bias_analysis_evaluation = combine_bias_analysis_evals(
        [eval.bias_analysis_evaluation for eval in metadata_evaluations]
    )
    overall_description_evaluation = combine_freeform_evaluations(
        [eval.description_evaluation for eval in metadata_evaluations]
    )
    return BatchStructuredMetadataEvaluation(
        mean_contextual_info_evaluation=mean_contextual_info_evaluation,
        mean_date_evaluation=mean_date_evaluation,
        mean_location_evaluation=mean_location_evaluation,
        mean_names_evaluation=mean_names_evaluation,
        mean_publication_info_evaluation=mean_publication_info_evaluation,
        mean_transcription_evaluation=mean_transcription_evaluation,
        overall_bias_analysis_evaluation=overall_bias_analysis_evaluation,
        overall_description_evaluation=overall_description_evaluation,
    )


def batch_evaluate_structured_metadata(
    llm_structured_metadatas: list[StructuredMetadata],
    human_structured_metadatas: list[StructuredMetadata],
    chat_bedrock_converse_kwargs: dict[str, Any],
) -> BatchStructuredMetadataEvaluation:
    """Evaluate multiple structured metadata items.

    Args:
        llm_structured_metadatas (list[StructuredMetadata]): Structured metadata items generated by LLM
        human_structured_metadatas (list[StructuredMetadata]): Structured metadata items generated by a human.
        chat_bedrock_converse_kwargs (dict[str, Any]): ChatBedrockConverse keyword args.

    Returns:
        BatchStructuredMetadataEvaluation: _description_
    """
    individual_evals: list[FreeformResponseEvaluation] = []
    for llm_structured_metadata, human_structured_metadata in zip(
        llm_structured_metadatas, human_structured_metadatas
    ):
        individual_eval = evaluate_structured_metadata(
            llm_structured_metadata=llm_structured_metadata,
            human_structured_metadata=human_structured_metadata,
            chat_bedrock_converse_kwargs=chat_bedrock_converse_kwargs,
        )
        individual_evals.append(individual_eval)

    return combine_structured_metadata_evaluations(individual_evals)
