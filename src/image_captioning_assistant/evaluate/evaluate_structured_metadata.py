"""Evaluate structured metadata generated by LLM."""

from typing import Any

from langchain_aws import ChatBedrockConverse
from loguru import logger
from pydantic import BaseModel, Field

from image_captioning_assistant.data.data_classes import StructuredMetadata
from image_captioning_assistant.evaluate.evaluate_bias_analysis import (
    BiasAnalysisEvaluation,
    evaluate_bias_analysis,
)
from image_captioning_assistant.evaluate.evaluate_freeform_description import (
    FreeformResponseEvaluation,
    evaluate_freeform_response,
)

PROMPT_TEMPLATE = """Compare the structured image metadata generated by an LLM
against the structured metadata written by a human. \
Evaluate the LLM's performance, assuming the human's answer is the gold standard.
<human_written_metadata>{human_written_metadata}</human_written_metadata>
<llm_generated_metadata>{llm_generated_metadata}</llm_generated_metadata>"""


class PartialStructuredMetadataEvaluation(BaseModel):
    """Evaluation of many of the structured metadata fields generated by an LLM."""

    transcription_evaluation: float = Field(
        ..., description="Score from 0-1 evaluating LLM's transcription of image text"
    )
    names_evaluation: float = Field(
        ...,
        description="Score from 0-1 evaluating named individuals LLM found in image",
    )
    dates_evaluation: float = Field(
        ..., description="Score from 0-1 evaluating date of image estimated by LLM"
    )
    location_evaluation: float = Field(
        ..., description="Score from 0-1 evaluating location of image estimated by LLM"
    )
    publication_info_evaluation: float = Field(
        ...,
        description="Score from 0-1 evaluating publication info for image generated by LLM",
    )
    contextual_info_evaluation: float = Field(
        ...,
        description="Score from 0-1 evaluating contextual info for image generated by LLM",
    )


class StructuredMetadataEvaluation(PartialStructuredMetadataEvaluation):
    """Evaluation of all structured metadata fields generated by an LLM."""

    description_evaluation: FreeformResponseEvaluation = Field(
        ..., description="Evaluation of LLM's description of image"
    )
    bias_analysis_evaluation: BiasAnalysisEvaluation = Field(
        ..., description="Evaluation of LLM's analysis of bias in image"
    )


def evaluate_structured_metadata(
    llm_structured_metadata: StructuredMetadata,
    human_structured_metadata: StructuredMetadata,
    chat_bedrock_converse_kwargs: dict[str, Any],
) -> StructuredMetadataEvaluation:
    """Evaluate structured metadata generated by an LLM.

    Args:
        llm_structured_metadata (StructuredMetadata): Structured metadata generated by an LLM.
        human_structured_metadata (StructuredMetadata): Structured metadata written by a human.
        chat_bedrock_converse_kwargs (dict[str, Any]): Keyword args for ChatBedrockConverse.

    Returns:
        StructuredMetadataEvaluation: Evaluation of the structured metadata the LLM generated.
    """
    # Evaluate freeform image description
    freeform_response_evaluation: FreeformResponseEvaluation = (
        evaluate_freeform_response(
            llm_freeform_response=llm_structured_metadata.description,
            human_freeform_response=human_structured_metadata.description,
        )
    )
    # Evaluate bias
    bias_analysis_evaluation: BiasAnalysisEvaluation = evaluate_bias_analysis(
        llm_bias_analysis=llm_structured_metadata.bias_analysis,
        human_bias_analysis=human_structured_metadata.bias_analysis,
    )
    # Build structured LLM client
    structured_llm = ChatBedrockConverse(
        **chat_bedrock_converse_kwargs
    ).with_structured_output(PartialStructuredMetadataEvaluation)

    # Build prompt
    prompt = PROMPT_TEMPLATE.format(
        human_written_metadata=human_structured_metadata.model_dump(),
        llm_generated_metadata=llm_structured_metadata.model_dump(),
    )
    logger.debug(f"Prompt:\n------------\n{prompt}\n------------")

    # Invoke LLM
    partial_structured_metadata_evaluation: PartialStructuredMetadataEvaluation = (
        structured_llm.invoke(prompt)
    )

    return StructuredMetadataEvaluation(
        description_evaluation=freeform_response_evaluation,
        transcription_evaluation=partial_structured_metadata_evaluation.transcription_evaluation,
        names_evaluation=partial_structured_metadata_evaluation.names_evaluation,
        dates_evaluation=partial_structured_metadata_evaluation.dates_evaluation,
        location_evaluation=partial_structured_metadata_evaluation.location_evaluation,
        contextual_info_evaluation=partial_structured_metadata_evaluation.contextual_info_evaluation,
        publication_info_evaluation=partial_structured_metadata_evaluation.publication_info_evaluation,
        bias_analysis_evaluation=bias_analysis_evaluation,
    )
