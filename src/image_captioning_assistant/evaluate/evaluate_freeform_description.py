"""Evaluate freeform description generated by LLM."""

from typing import Any

from langchain_aws import ChatBedrockConverse
from loguru import logger
from pydantic import BaseModel, Field

PROMPT_TEMPLATE = """Compare an LLM's answer against a human's answer \
and score the LLM's answer, assuming the human's answer is the gold standard.
<human_response>{human_response}</human_response>
<llm_response>{llm_response}</llm_response>"""


class FreeformResponseEvaluation(BaseModel):
    """Evaluation of an LLM's freeform response"""

    faithfulness_and_consistency: float = Field(
        ...,
        description="Score from 0-1 measuring the faithfulness and consistency of the LLM's response.",
    )
    completeness: float = Field(
        ...,
        description="Score from 0-1 measuring the completeness of the LLM's response.",
    )
    verbosity: float = Field(
        ...,
        description="Score from 0-1 measuring the verbosity of the LLM's response.",
    )
    clarity: float = Field(
        ...,
        description="Score from 0-1 measuring the clarity of the LLM's response.",
    )


def evaluate_freeform_response(
    llm_freeform_response: str,
    human_freeform_response: str,
    chat_bedrock_converse_kwargs: dict[str, Any],
) -> FreeformResponseEvaluation:
    """Evaluate a freeform

    Args:
        llm_freeform_response (str): Freeform response generated by LLM.
        human_freeform_response (str): Freeform response generated by human.
        chat_bedrock_converse_kwargs (dict[str, Any]): Keyword args for ChatBedrockConverse.

    Returns:
        FreeformResponseEvaluation: Evaluation of LLM's response.
    """
    # Build structured LLM client
    structured_llm = ChatBedrockConverse(
        **chat_bedrock_converse_kwargs
    ).with_structured_output(FreeformResponseEvaluation)

    # Build prompt
    prompt = PROMPT_TEMPLATE.format(
        llm_response=llm_freeform_response,
        human_response=human_freeform_response,
    )
    logger.debug(f"Prompt:\n------------\n{prompt}\n------------")

    # Invoke LLM
    evaluation: FreeformResponseEvaluation = structured_llm.invoke(prompt)

    return evaluation
